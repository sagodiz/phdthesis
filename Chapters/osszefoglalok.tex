\chapter*{Összefoglalás}
\markboth{Összefoglalás}{Összefoglalás}
\addcontentsline{toc}{chapter}{Összefoglalás}

A Szoftverfejlesztés egy gyorsan fejlődő terület.
A fejlesztőknek eddig is volt és ezután is lesz igényük a forráskód elemzésre, legyen ez az igény direkt vagy indirekt módon kifejezve.
Direkt mód alatt érthetjük a kód minőségének mérését vagy esetleg a sérülékenységek detektálását.
Az indirekt módba tartoznak azok az eszközök melyek a háttérben használják a forráskód elemzést, pl. IDE-k vagy automatikus kódjavító eszközök.

A nagy nyelvi modellek a statikus elemző eszközök egy speciális típusának tekinthetők, mivel elegendő közvetlenül a forráskódot megadni, fordítási lépés nélkül.
Az LLM-ek egyre inkább beépülnek a szoftverfejlesztés kulcsfontosságú folyamataiba, beleértve a tervezést, a dokumentációt és a feladatkezelést.
Bár ezek a modellek számos területen alkalmazhatók, jelen dolgozat kifejezetten a forráskód-szintézis egyik formájára fókuszál.

A dolgozat célja a fejlesztők és a nagy nyelvi modellek közötti kapcsolat feltárása és elmélyítése, hogy a nyelvi modellek a fejlesztők végső fejlesztéstámogató eszközeivé válhassanak.
A disszertáció számos olyan aspektusát vizsgálja az LLM-eknek, amelyek a fejlesztők számára hasznosak lehetnek, különös tekintettel az összehasonlítási technikákra és a hiperparaméterek kiemelt szerepére.
A munka nem csupán elméleti megközelítést ad, hanem egy aktuális állapotjelentést is nyújt arról, hogy mik a reális elvárások ezekkel a modellekkel szemben.

\section*{Tézis I.}

A \ref{chapter_2}. fejezetben egy összehasonlításhoz használható metodólógiát alakítottunk ki azon fellelhető szakirodalom alapján, melyek valamilyen módon LLM-eket hasonlítottak össze.
Ezeket a cikkeket elemezve, olyan rendszert alakítottunk ki, mely segítségével két LLM teljeskörűen összehasonlítható.
Arra is kitérünk, hogy az egyes lépéseket hogyan kell kivitelezni és mire kell figyelmet fordítani.

A folyamat a megfelelő propmt kiválasztásával kezdődik.
A jólmegszokott eszközökkel szemben, az LLM-eket csakis a megfelelő prompt segítségével tudjuk a legjobb minőségben kihasználni, ezért fontos, hogy kiválasszuk a feladathoz és a modellhez illő legjobb promptot.

Miután kiválasztottuk a promptot, a modellek funkcionális tesztelése következik, azaz a modellek által generált kód valóban azt csinálja-e amit kell.
A legnehezebb feladat ebben a lépésben, hogy lemérjük, a kód valóban azt csinálja amit kell.
A legegyszerűbb módja az, ha automatizált teszteket használunk, azonban nagyon fontos figyelni arra, hogy ezek a tesztek gyakran nem tudják az LLM generálta kódot felhasználni, így valamilyen teszt keretrendszert kell alkalmaznunk.
Az, hogy kell-e kiegészítő rendszereket használnunk, módosítanunk a kódon attól függ, hogy mi az elvárt kimenet.
Ha a fealdat része, hogy a kódnak az adott tesztekkel kompatibilisnek kell lenniük, akkor nem szükséges további módosítást végeznünk.

A tesztek beszerzése sem egyszerű feladat, hiszen vagy a fejlesztő csapatnak kell saját teszteket alkotniuk, vagy már meglévő kódbázist használhatnak a tesztek során, ekkor azonban fennáll a lehetősége, hogy a modellek rátanulnak a publikusan elérhető kódokra.
Bármely megoldást is választjuk, megfelelő minőségű tesztesetet szerezni nem könnyű feladat.

Harmadik lépésként a nem funkcionális tesztelésen avagy a technikai minőségmérésen van a sor.
Ez a lépés biztosítja azt, hogy az LLM által generált kód minpségében legalább olyan jó mint az eddig meglévő kódbázis.

Az utolsó lépésként az összehasonlítási lépésekhez bevettük az emberi kiértékelést, annak érdekében, hogy látható legyen, a fejlesztők hajlandók-e elfogadni a modellek által generált kódot.
Habár ez a lépés költséges és bonyolultan kivitelezhető, nagyon fontos, hiszen végsősoron a fejlesztőknek kell együttdolgozniuk a modellekkel.

A metodológia felállítása után egy esettanulmányban megmutattuk, hogyan is használatos a módszertanunk.
Megjegyezzük azt is, hogy extra kiértékelési szempontok is bevethetők, legyen az teljesítmény mérés vagy komplexitás vizsgálat, attól függően, a fejlesztőcsapatnak milyen igényei vannak.


\section*{Work II.}

In Chapter~\ref{chapter_3}, We created a dataset, that contains 18,900 raw and 18,896 processed LLM outputs in C++ source code generation.
Inferencing LLMs especially larger ones requires special hardware which might not be avaliable for everyone who wants to research LLMs.
To broaden the possibilities of the research community, we evaluated 9 LLMs from 3 different LLM-families with various model sizes, including models with the size of 70 billion parameters.
These models were inferred with variadic temperature settings ranging from 0.01 to 1.00.

The creation of this dataset did not include randomly selected values.
We meticulously researched available open-source models and selected the best ones according  to multiple benchmarks.
As the models were selected with great attention, the tasks to be generated are also considered with great attention.

We needed tasks that provide a meaningful task for the LLMs, meaning they are not trivial to understand and implement.
These tasks also had to be hidden from the major benchmarks of LLMs, therefore, reducing the risk of overfitting.
To achieve this goal, we used tasks from Sapientia ECN, a programming competition.
This competition guarantees that the tasks are non-trivial and since the tasks are hand-made by the organizers the originality of the tasks is also guaranteed.

These tasks were provided to the LLMs alongside our prompt which we created considering good prompting practices such as role specification or few-shot learning.
The LLMs were inferred through our inference framework.

We also validated the generated outputs by compiling them.
Ones that compiled successfully were taken as successful generations, although ones that failed the compilations had to be checked.
We manually checked a statistically significant amount of those examples, and found that there were no mistakes in our framework, only the LLMs failed to generate source code that compiles.

The examples that could be compiled were also evaluated with the available testcases from the programming competition.
We collected these results in a JSON file which we published.

Using this JSON file We also presented, that the temperature hyper-parameter of LLMs does not hide a pattern for source code generation and this topic requires more research.

\section*{Work III.}

Intro

In Chapter~\ref{chapter_4}, We took an alternative form of source code generation and used to generate fixed source code to perform so called automatic program repair.
We used GPT-4 to generate the source code in a way that maintains the original functionality without the hidden vulnerability inside.

This task is evaluated on a real-life vulnerability benchmark, Vul4J.
This benchmarks contains tests that validate whether the plausible fix correctly removes the vulnerability or not.
To prevent our results to be influenced by GPT-4 being familiar with the vulnerabilities collected in the benchmark, We also used a benchmark, which had no validation tests, thus We could check correctness only by hand, that has vulnerabilities only after the training period of that particular GPT-4 version.

We created our prompt by considering multiple prompting strategies and we selected the best performing on a sub-set of Vul4J.
With our prompt, we prompted GPT-4 to fix the source code and also provide a textual answer.
This two sided request allowed us to discover if GPT can be a useful guide even if it cannot provide a valid vulnerability fix.

\newpage
\section*{Future Work}

Overall, this thesis highlights an important segment of the broader topic of LLMs in software engineering, while also indicating directions for future research.
In Chapter~\ref{chapter_2}, optional comparison criteria could be incorporated, and an automated system might be developed to perform functional and non-functional tests.
While human evaluation cannot be fully automated, a substantial portion of the process could be streamlined.

In Chapter~\ref{chapter_4}, the patterns associated with the temperature hyper parameter can be further explored at multiple levels.
This includes not only understanding its effects on model behavior but also determining the optimal granularity for its application.

Although Chapter~\ref{chapter_3} may appear complete, given that GPT-4 is now considered an older model, there remains room for further work.
Newer models could be evaluated, and improved inference techniques could be developed to achieve higher success rates.
While the one-third success rate observed is promising, real-world developers require even more reliable outcomes.

\vspace{5em}
To conclude this thesis, Large Language Models have become an integral part of software engineering, but they still require extensive research across multiple areas and levels to fully meet the needs of the developer community and the developer community needs to understand and adapt to these new tools.


\vfill
\pagebreak

\section*{Contributions of the thesis}

In the \textbf{first thesis group}, the contributions are related to Large Language Model comparison in software synthesis. Detailed discussion can be found in Chapter~\ref{chapter_2}.

\begin{enumerate}[wide = 0pt, widest = {I/4.}, leftmargin =*]
	
	\item[I/1.] I collected relating methodologies and determined the gaps in those methodologies.
	
	\item[I/2.] I created the comparison methodology.
	
	\item[I/3.] I searched for an evaluation benchmark.
	
	\item[I/4.] I evaluated the Language Models on the benchmark.
	
	\item[I/5.] I performed the quality analysis of the synthetized programs.
	
	\item[I/6.] I designed the human evaluation.
	
	\item[I/7.] I created the Human Evaluation framework.
	
\end{enumerate}

\vspace{1cm}

\noindent
In the \textbf{second thesis group}, the contributions are related to creation of a dataset and the hyper parameter investigation of Large Language Models in software synthesis. Detailed discussion can be found in Chapter~\ref{chapter_3}.

\begin{enumerate}[wide = 0pt, widest = {II/5.}, leftmargin =*]
	
	\item[II/1.] I designed the 488study.
	
	\item[II/2.] I created the model selection framework.
	
	\item[II/3.] I created the model evaluation framework.
	
	\item[II/4.] I evaluated the data according to the temperature variable effect.
	
\end{enumerate}

\vspace{1cm}

\noindent
In the \textbf{third thesis group}, the contributions are related to the evaluation of GPT-4 in real-life vulnerability fixing. Detailed discussion can be found in Chapter~\ref{chapter_4}.

\begin{enumerate}[wide = 0pt, widest = {III/5.}, leftmargin =*]
	
	\item[III/1.] I searched for prompt engineering techniques.
	
	\item[III/2.] I designed the evaluation of the prompts and also the models.
	
	\item[III/3.] I participated in the manual evaluation of the generated source code.
	
	\item[III/4.] I participated in the manual evaluation of the generated textual responses.
	
\end{enumerate}

The author states that while the thesis results are primarily his own work, the
pronoun we is used instead of I to recognize the input of co-authors in the papers
forming the basis of this thesis.
