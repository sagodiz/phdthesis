\chapter*{Summary}
\markboth{Summary}{Summary}
\addcontentsline{toc}{chapter}{Summary}

Software engineering is a rapidly evolving field.
Developers have demonstrated, and will continue to demonstrate, a persistent need for source code analysis, both in direct and indirect forms.
Direct forms include tools that assess code quality or detect vulnerabilities, while indirect forms encompass applications that leverage such analyses internally, such as IDEs or automated program repair (APR) tools.

Large Language Models can be considered a type of static analysis tool, as they can operate directly on source code without requiring compilation.
LLMs are increasingly integrated into key software engineering processes, including design, documentation, and task management.
Although these models are applied in diverse areas, this thesis focuses specifically on a form of source code synthesis.

The aim of this thesis is to discover and deepen the connection between developers and LLMs as an ultimate development aid.
This thesis is designed to discover many aspects of LLMs which could be useful for a developer, regarding possible comparison techniques and the highlight of hyper-parameters.
This thesis also provides an actual status report not just an abstract idea on what the realistic expectations are.


\section*{Work I.}

In Chapter~\ref{chapter_2}, We created a comparison methodology based on various papers that compare LLMs.
This way We came-up with a methodology, that takes multiple aspects into consideration not only a few.
The methodology details how every step should be performed what must be taken into consideration.

The methodology starts with the right prompt selections.
In contrast to methods We are all got used to, LLMs cannot be used without proper prompting.
The first thing We must do, is to select the right prompt for the model, for the task We are going to use the model for.

After selecting the right prompt, we must evaluate the functional validity of the models.
Functional validity simply said, measures if the model can provide such code that does what it should do.
The hardest task at this point is to find a way, how can We measure ,,it does what it should do''.
It is most likely be performed via automatized tests, although it is important to note, that these tests might not take the generated code as is, so a framework might be needed.
Using a framework or not completely depends on the requirements, as the requirements might contain that the code must be testable by the existing tests.

Getting the tests is not an easy task either, as either it is up to the team to create the tests or use existing benchmarks, although at that case overfitting must be taken into consideration.
Either way, tests are not easy to get in good quality.

At the third step of the methodology, we must evaluate the non-functional validity or technical quality.
This step provides that the introduced LLM maintains the source code quality or rather increases it.

As our last step, We included the human evaluation, in order to see if developers are willing to accept source code generated by LLMs.
This step, although difficult and expensive to perform, provides a better overview if an LLM should be introduced to the developer team or not.

As an additional step, We done a case study to provide an actual example, how to use our methodology.
We also note, that additional evaluations might be added, such as complexity or performance tests, depending on the needs of the development team.

\section*{Work II.}

In Chapter~\ref{chapter_4}, We created a dataset, that contains 18,900 raw and 18,896 processed LLM outputs in C++ source code generation.
Inferencing LLMs especially larger ones requires special hardware which might not be avaliable for everyone who wants to research LLMs.
To broaden the possibilities of the research community, we evaluated 9 LLMs from 3 different LLM-families with various model sizes, including models with the size of 70 billion parameters.
These models were inferred with variadic temperature settings ranging from 0.01 to 1.00.

The creation of this dataset did not include randomly selected values.
We meticulously researched available open-source models and selected the best ones according  to multiple benchmarks.
As the models were selected with great attention, the tasks to be generated are also considered with great attention.

We needed tasks that provide a meaningful task for the LLMs, meaning they are not trivial to understand and implement.
These tasks also had to be hidden from the major benchmarks of LLMs, therefore, reducing the risk of overfitting.
To achieve this goal, we used tasks from Sapientia ECN, a programming competition.
This competition guarantees that the tasks are non-trivial and since the tasks are hand-made by the organizers the originality of the tasks is also guaranteed.

These tasks were provided to the LLMs alongside our prompt which we created considering good prompting practices such as role specification or few-shot learning.
The LLMs were inferred through our inference framework.

We also validated the generated outputs by compiling them.
Ones that compiled successfully were taken as successful generations, although ones that failed the compilations had to be checked.
We manually checked a statistically significant amount of those examples, and found that there were no mistakes in our framework, only the LLMs failed to generate source code that compiles.

The examples that could be compiled were also evaluated with the available testcases from the programming competition.
We collected these results in a JSON file which we published.

Using this JSON file We also presented, that the temperature hyper-parameter of LLMs does not hide a pattern for source code generation and this topic requires more research.

\section*{Work III.}

In Chapter~\ref{chapter_3}, We took an alternative form of source code generation and used to generate fixed source code to perform so called automatic program repair.
We used GPT-4 to generate the source code in a way that maintains the original functionality without the hidden vulnerability inside.

This task is evaluated on a real-life vulnerability benchmark, Vul4J.
This benchmarks contains tests that validate whether the plausible fix correctly removes the vulnerability or not.
To prevent our results to be influenced by GPT-4 being familiar with the vulnerabilities collected in the benchmark, We also used a benchmark, which had no validation tests, thus We could check correctness only by hand, that has vulnerabilities only after the training period of that particular GPT-4 version.

We created our prompt by considering multiple prompting strategies and we selected the best performing on a sub-set of Vul4J.
With our prompt, we prompted GPT-4 to fix the source code and also provide a textual answer.
This two sided request allowed us to discover if GPT can be a useful guide even if it cannot provide a valid vulnerability fix.
Our findings show that in real-world scenario the GPT-4 model can fix 33.33\% of the vulnerabilities  and can provide useful answers in around 50\% of the cases.

\newpage
\section*{Future Work}

Overall, this thesis highlights an important segment of the broader topic of LLMs in software engineering, while also indicating directions for future research.
In Chapter~\ref{chapter_2}, optional comparison criteria could be incorporated, and an automated system might be developed to perform functional and non-functional tests.
While human evaluation cannot be fully automated, a substantial portion of the process could be streamlined.

In Chapter~\ref{chapter_4}, the patterns associated with the temperature hyper parameter can be further explored at multiple levels.
This includes not only understanding its effects on model behavior but also determining the optimal granularity for its application.

Although Chapter~\ref{chapter_3} may appear complete, given that GPT-4 is now considered an older model, there remains room for further work.
Newer models could be evaluated, and improved inference techniques could be developed to achieve higher success rates.
While the one-third success rate observed is promising, real-world developers require even more reliable outcomes.

\vspace{5em}
To conclude this thesis, Large Language Models have become an integral part of software engineering, but they still require extensive research across multiple areas and levels to fully meet the needs of the developer community and the developer community needs to understand and adapt to these new tools.


\vfill
\pagebreak

\section*{Contributions of the thesis}
\input{./Chapters/contributions}
