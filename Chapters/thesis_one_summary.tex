We created a comparison methodology based on various papers that compare LLMs.
This way We came-up with a methodology, that takes multiple aspects into consideration not only a few.
The methodology details how every step should be performed what must be taken into consideration.

The methodology starts with the right prompt selections.
In contrast to methods We are all got used to, LLMs cannot be used without proper prompting.
The first thing We must do, is to select the right prompt for the model, for the task We are going to use the model for.

After selecting the right prompt, we must evaluate the functional validity of the models.
Functional validity simply said, measures if the model can provide such code that does what it should do.
The hardest task at this point is to find a way, how can We measure ,,it does what it should do''.
It is most likely be performed via automatized tests, although it is important to note, that these tests might not take the generated code as is, so a framework might be needed.
Using a framework or not completely depends on the requirements, as the requirements might contain that the code must be testable by the existing tests.

Getting the tests is not an easy task either, as either it is up to the team to create the tests or use existing benchmarks, although at that case overfitting must be taken into consideration.
Either way, tests are not easy to get in good quality.

At the third step of the methodology, we must evaluate the non-functional validity or technical quality.
This step provides that the introduced LLM maintains the source code quality or rather increases it.

As our last step, We included the human evaluation, in order to see if developers are willing to accept source code generated by LLMs.
This step, although difficult and expensive to perform, provides a better overview if an LLM should be introduced to the developer team or not.

As an additional step, We done a case study to provide an actual example, how to use our methodology.
We also note, that additional evaluations might be added, such as complexity or performance tests, depending on the needs of the development team.
