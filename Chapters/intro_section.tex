\chapter{Introduction}

%Software Engineering is an ever growing and evolving field.
%Not only are there many and many new fields, but the technology behind it is a constantly and rapidly changing topic.
%Since the manually created software, We got to a point where the Integrated Development Environments (IDEs),- which are a huge help for developers by the syntax highlight, navigation, formatting and so on features,- are included in every developers arsenal.
%These tools were created for the convinience of the  developers, although many has failed and they are constantly changing by the requirements of the developer community.

%Such tools are the recently introduced Artificial Intelligence (AI) based Large Language Models (LLMs) which are currently transforming our jobs as software engineers.
%Similarly, these tools and models require adaptation to developer requirements and also, the developers must discover what these tools are capable of, what are the main tasks which can be performed on a higher quality or same quality but faster by using them.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Software engineering is a continuously growing and evolving field.
Not only do new subfields emerge constantly, but the underlying technologies are also changing at a rapid pace.
From the early days of manually written software, we have progressed to a point where Integrated Development Environments (IDEs)—which provide substantial support through features such as syntax highlighting, code navigation, and automated formatting—have become standard tools in every developer’s arsenal.
Although these tools were created for developer convenience, many have failed over time, and those that remain continue to evolve in response to the changing requirements of the developer community.

IDEs rely on a variety of analysis techniques, with source code analysis being a central component.
While some features may appear straightforward, such as navigating from a caller to a callee, they often depend on extensive research, such as the construction of call graphs.
Building call graphs is a complex task, as demonstrated in several of our studies~\cite{cg1, cg2, cg3}.

Just as a small subset of IDE features is underpinned by a substantial research foundation, newly developed tools also require a solid research base to ensure their effectiveness and reliability.
Such tools are the recently introduced Large Language Models (LLMs), which are currently reshaping the daily work of software engineers.
As with earlier tools, these models must adapt to developer needs, and developers, in turn, must explore their capabilities to determine which tasks can be performed at higher quality—or at the same quality more efficiently—through their use.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%Developers are facing many difficulties such as the need of change in coding habits and the way how creative work becomes mostly a review work by the appliance of AI models.
%In this work another difficulties are highlighted which are mostly considered from a technical point.
%With LLMs popping out daily developers cannot choose the right models for their work.
%Experiencing with the most hyped models takes time and could miss potential optimal models.
%It is a problem, that the newly presented tools or models do not have a common base of quality measurement, a way of comparing them, therefore, one model cannot be confidently preferred over another.

%Another technical problem is that although a few benchmarks are available, developers cannot completely trust in them, as the benchmarks either consist of smaller non-real-life scenarios or they are prone to overfitting on the benchmarks for better evaluation.
%Developers must know, what is the real current level of LLMs in a real-life scenario.

%Yet another problem is that these models are configured by many hyper-parameters, which provide a great customization to developers once they select the well-performing model for their usage, however, they cannot know how to configure for their task the model even if there is any pattern in configuring various models.
%Although generic information is available on many of such hyper-parameters in a larger scale, but as a daily user developers might not benefit or even notice these differences.

Developers today face numerous challenges, including the need to change established coding habits and adapt to a workflow in which creative work is increasingly transformed into review-oriented work through the use of AI models.
This work also highlights additional challenges that are primarily technical in nature.
With new LLMs appearing almost daily, developers struggle to select the most suitable model for their tasks.
Experimenting with the most popular models is time-consuming and may lead them to overlook potentially optimal alternatives.
A further issue is that newly introduced tools and models lack a common, standardized framework for quality measurement and comparison; as a result, developers cannot confidently prefer one model over another.
Although various benchmarks are available, those mostly measure the functionality of the generated code.
Even with versatile benchmarks, developers have to find those benchmarks and often evaluate the models on them as most of the models are not evaluated from the perspectives of source code quality, security, or other non-functional criteria.

As benchmarks are mentioned, another technical challenge is that, although some benchmarks are available, developers cannot fully trust them.
These benchmarks often rely on small, non–real-world scenarios.
Such benchmarks do not provide actual information about how models would perform when applied in actual development.
It is also a problem that these benchmarks are prone to overfitting, as models may be optimized specifically for better benchmark performance.
As model trainers know, which benchmarks are checked for quality measures, those benchmarks could be placed multiple times in the training set in order to seemingly increase the performance.
Consequently, developers need reliable insight into the true real-world capabilities of LLMs.

An additional problem is that these models are controlled by numerous hyper-parameters.
While these offer extensive customization once a well-performing model has been selected, developers often lack guidance on how to configure the model effectively for their specific tasks or whether consistent configuration patterns exist across different models.
Although general information about many hyper-parameters is available at a broader level, everyday users frequently cannot benefit from, or may not even notice, their impact in practice.

%This dissertation discusses the above mentioned three challenges in order.
%Although these challenges can be addressed through multiple fields of software engineering, source code generation, source code quality, and source code security are the main topics.
%Generation and quality is the base of all workflows as without code, there is no software and quality control is a commonly performed task in larger projects.
%Security on the other hand is not a new topic, however, definitely getting more and more relevant as the security incidents are growing day-by-day.

While these challenges may be approached from multiple subfields of software engineering, the primary focus of this work is on source code generation, source code quality, and source code security.
Source code generation and quality form the foundation of all software development workflows, as software cannot exist without code, and quality assurance is a routinely performed and essential activity in large-scale projects.
Source code security is a well-established research area; however, its importance continues to increase as the frequency and impact of security incidents grow steadily over time.

This dissertation addresses the three aforementioned challenges through three thesis points, see \autoref{tbl:thesispoints}. The \textit{first thesis point}, presented in \autoref{chapter_2} focuses on the problem of LLM comparison, outlining the essential evaluation steps and the main pitfalls. The \textit{second thesis point}, described in \autoref{chapter_4} investigates hyper-parameter usage and overfitted benchmarks, introducing a dataset to show that temperature remains an underexplored factor in mastering LLM-based code generation. Finally, the \textit{third thesis point}, detailed in \autoref{chapter_3} evaluates GPT-4 on vulnerability fixing in a real-world context, providing a clearer view of current model capabilities.

\begin{table}[!h]
	\begin{tabular}{|l|p{0.8\linewidth}|c|}
		\hline
		\textbf{\#} & \textbf{Thesis}       &  \textbf{Chapter} \\ \hline
		1           &  \thesispointone.      &  3                \\ \hline
		2           &  \thesispointtwo.      &  4                \\ \hline
		3           &  \thesispointthree.    &  5                \\
		\hline
	\end{tabular}
	\caption{List of thesis points}
	\label{tbl:thesispoints}
\end{table}
